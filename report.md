# 拼音输入法 报告
> 刘晓义 2017011426

本文件包括实验报告。关于程序的运行方法，请参考 README.md

## 算法实现基本思路

输入法采用以词为单元的 2-gram + 3-gram，模型类似 HMM，使用略加修改的 Viterbi 算法求解。

在 10000 条随机选出的句子中，共能够达到 96% 的正确率。与 2-gram 以及简单词频预测的比较在下方与其他算法的比较以及测试结果两节中有阐述。

### 算法细节

核心算法框架是 Viterbi，由于每个状态对应的用于比较的值不是严格意义上的频率/概率，因此一下都用`权`表示。

和 Viterbi 不同的部分在于：对每一轮得到的状态进行了合并，以及标准化。
- 合并是合并了所有输出相同的状态，这在 3-gram 中可能由于词语的切分不同而出现，例如 (今天,出去玩) 和 (出去, 玩) 可能对应同样的输出。这个操作解决的问题是，有时出现很少的词汇组合因为拆分稳定而超过了拆分不稳定的词汇组合。
- 标准化是按比例缩放所有状态使得他们的权加成等于 1。 原因是对于后续某个位置计算转移的时候，由于词的长度可能不同，转移来源的前缀可能不同。标准化可以保证每个前缀的状态对应的权基本意义一致，而且也可以抹去参数量纲的影响。

此外，为了保证就算一个组合没有出现也不会被完全排除，避免完全没有输出的情况，加入了 Laplace 平滑 (感谢 最强大脑郑林楷 的建议)

```
- 前缀的转移状态 states: Vec<HashMap<(String, String), f64>>, states[i] 表示前缀 seg[0..i) 的状态以及对应的权
- 前缀的输出 paths: Vec<HashMap<(String, String), String>>, paths[i] 表示前缀 seg[0..i) 的状态对应的输出
- 初始化: states[0] = {("", ""): 1}
- 初始化: paths[0] = {("", ""): ""}
- 对于每个拼音串的前缀 seg[0..n), n in [1, len(seg)]：
  - 初始化当前状态和输出 state, path
  - 枚举这一前缀的全部后缀 seg[from..n)，找到所有有对应读音的词(通过一个倒过来的 Trie)。对于每个词 word：
    - 应该有：len(word) = n - from
    - 枚举 states[from] 中的所有状态: (a, b): weight
      - 计算出转移权 transfered_weight =weight * transfer(a, b, word)，其中转移权按照不同比例考虑了 3-gram, 2-gram 以及简单的词频
    - 得到最大的 transfered_weight: max_weight 以及对应的状态 (max_a, max_b)
    - 在当前状态中记录： state[(b, word)] = transfered_weight
    - 在当前输出中记录： path[(b, word)] = paths[from][(a, b)] + word
  - 合并所有 path[s] 相同的 s，权值求和，剩下的状态是原来权值最大的状态。
  - 对 state 进行标准化: forall i, state[i] /= sum(state)
  - states[n] = state, paths[n] = path
- 选出 states[len(seg)] 中最大的状态 s, 结果就是 paths[len(seg)][s]
```

训练算法比较显然：
- 读取训练数据，解析
- 去除不包含在输入法内的字(英文、数字、字表以外的字)
- 分词
- 根据转移表大小，筛选出最经常出现的三元对，保存进训练结果里

选择的最大转移表大小是 50M，提交版本的转移表大小大约是 20M，由于被我电脑的内存限制住了...

测试的算法也比较显然，就是输入后去掉有非法字符的句子，通过 rust\_pinyin 转换成 pinyin。由于算法中没有考虑拼音本身出现的频率，所以多音字只要随便选一个音就可以了。最后把算法输出和原输入比较。

### 与其他算法的比较

总共依次尝试过一下四种不同的算法：
- 字的 2-gram
- 词的 2-gram
- 词的 3-gram
- 词的 2-gram + 3-gram

从字切换到词的原因是：相邻的两三个字一般位于同一个词内，所以根据字进行预测的时候每个字的影响范围大概就在所在的词语中。因此这样效果和直接根据词频做预测一样，而且会丢失句子的语义。

例如测试数据中：`两会在北京召开`

由于`会在`一词经常出现，因此模型对于拼音(hui, zai)非常敏感，而`liang 会`读音的词都比较少出现，因此最后的结果就是第一个字基本上是根据字频做的预测。而`量`比`两`出现次数要多，结果就是输出是 `量会在北京召开`

事实上，从字的 2-gram 切换到词的 2-gram 后，效果有显著提升。比如以上例子，由于分词中`在`经常作为单独的一个词出现，而`两会`如果出现就肯定会分到一起，同时根据 2-gram 预测，作为地点的 `北京` 经常作为 `在` 的后一个词，所以可以正确输出 `两会在北京召开`。

从词的 2-gram 切换到 3-gram 的原因是加强对语境语义的重视，还有对异常数据的容忍。由于在训练时使用 Jieba 分词，并没有打开 HMM 开关，因为如果打开了会把专有词汇合在一起，例如清华大学会作为一个词，这样词库会太大，存不下。但是如果关闭了就会导致部分词被切分的太细，变成了单字，这样以词作为基础的优势就消失了。

比如 `拼音输入法` 一句，2-gram 给出的结果会是 `拼音朱如法`，`测试一下`会变成`侧是以夏`，推测是因为训练数据中有类似的被切分太细的词。由于 `一下` 会被切成同一个词，而 `以夏` 会被当成两个词，而 `xx 一下` 的可能性太多了，同时作为异常出现的 `以夏` 出现的少，但是相对于其他可能，概率会非常高，最后模型选择了 `以夏`。

换成 3-gram 之后效果有明显改善，由于现代汉语多为双音节，3-gram 会使得即使切分过细也可以至少跨越两个词，带有一定的语义信息。

3-gram 再加上 2-gram 的原因是：对于句首、句尾以及两个单词组成的词组的补偿。如果输入的某个部分对应符合读音的 tuple 没有任何一个出现在 3-gram 训练集中，那么就算有 Laplace 平滑的处理，这一段转移出来的下一个词也会是随机一个符合读音的词，因为大家预测出来的概率都一样。

例如 `后天网上冲浪` 一词，这个组合在训练集中完全没有出现，虽然后天、网上都是概率比较高的词，初始化的时候会作为种子填进去，但是第三个词完全没办法预测，随机得到的了一个，最后的结果就是 `后天上网重郎`

为了补偿这一点，对于没有前两个词转移出来的第三个词的情况，根据前一个词转移做 2-gram，也可以得到一个概率比较高的词。在上面的例子中，没有任何符合 `hou tian wang shang chong lang` 的三元对，但是存在 `网上 + 冲浪` 的高频二元对。第一个词虽然在这种情况下没办法根据转移而预测，但是在初始化中根据词频做预测时，`后天` 就是这个读音内频率最高的词汇。最后得到正确结果 `后天网上冲浪`

在实现中，这点的是通过一下方法完成的：

```
weight = (词频 + LAPLACE) * 1_GRAM_FACTOR + (2_gram + LAPLACE) * 2_GRAM_FACTOR + (3_gram + LAPLACE) * 3_GRAM_FACTOR
```

考虑到在算法每一个为止最后还要进行一词标准化，所以式子中的大写常量参数取值量纲是没有关系的。在提交版本代码中，参数如下选择：

```
- LAPLACE = 0.001
- 1_GRAM_FACTOR = 1000
- 2_GRAM_FACTOR = 1000 * 100 = 1e5
- 3_GRAM_FACTOR = 1000 * 1000000000 = 1e12
```

具体对参数调整的讨论见下面 `参数调整` 一节。

### 实现过程
使用 Rust 实现

Rust 是很好的语言，可以自动为所有类型实现 Hash 和 (De)serializer(来自 serde)，因此所有这些 Boilerplate 都省去了。可以直接对 tuple 和其他类型放进 HashMap 里

命令行参数的解析部分使用了 Rust 的库 StructOpt (感谢 杰哥陈嘉杰 的推荐)，这个 Boilerplate 也省去了。

最终把共享的算法和数据结构拆开放到 crate 的根里，包括:
- 训练状态 TrainingStore
- 词典 Dict
- Home-made Trie
- 算法引擎 Engine

然后三个不同的 binary: trainer, main, tester 分别进行训练、输入法的运行、批量测试

## 效果

训练数据来自：提供数据中一部分的新闻，爬虫爬取的随机 Wikipedia 条目，以及我最近看过的还没来及删除的小说...总计大小 344M

在上述训练以及参数设置下，进行一万组测试，算法可以得到 **96.17%** 的正确率。

效果比较好句子一般都是完整没有省略，比较非口语化的:
```
> qing hua da xue ji suan ji ke xue yu ji shu xi
Out: 清华大学计算机科学与技术系
> ce shi xiao guo hai bu cuo
Out: 测试效果还不错
> shu chu fei chang wen ding
Out: 输出非常稳定
```

这件事情是很合理的。因为算法设计中就比较注重专有名词和上下文关联，这种句子是由专有名词、经常出现的词组或者出现频率很高的词作为主干，中间穿插介词。算法会确定高频词，之后根据上下文得到对应合理的介词和低频词。

相比之下表现比较差得，就是比较口语化的。这样的句子通常比较跳跃，而且有训练中少见的词汇组合：

```
> ni zai xia shuo shen me
Out: 你在下说什么
> xia shuo
Out: 瞎说
> a wo shou bu liao le
Out: 阿沃受不了了
> wo shou bu liao le
Out: 我受不了了
> wo men jia li yang de nv peng you chao ke ai
Out: 我们家里养的女朋友超科埃
> chao ke ai
Out: 超可爱
```

可以看到，单独输入一个词可以正确得到结果，但是如果放进句子里，反而会被上下文预测而影响。还有一个可能的问题来源在于分词的时候，对于这种句子难于处理。

解决这一点可能需要更强大的语义理解（对于词性的预测，从而保证句子结构正常），或者改变输入数据包含更多口语化的数据。

## 参数调整

考虑一下几种参数调整：
```
- 仅使用 3-gram: 2_GRAM_FACTOR = 0
- 仅使用 2-gram: 3_GRAM_FACTOR = 0
- 仅使用词频 : 2_GRAM_FACTOR = 3_GRAM_FACTOR = 0
- 关闭 Laplace 平滑: LAPLACE = 0
- 提高 2-gram 权重: 2_GRAM_FACTOR = 1e9
```

测试使用相同的 10000 组新闻数据：

| 调整 | 测试正确率 | Diff |
|------|------------|------|
| 不作调整 | 96.17% | 0 |
| 仅 3-gram | 96.16% | -0.01% |
| 仅 2-gram | 72.77% | -23.40% |
| 仅词频 | 4.44% | -91.73% |
| 关闭 Laplace 平滑 | 96.09% | -0.08% |
| 提高 2-gram | 95.87% | -0.30% |

可见 关闭 2-gram 以及关闭 Laplace 平滑在新闻数据中没有特别大的影响。其实这两者都是主要用于罕见词汇组合做处理，也就是更针对口语环境，这个结果也是合理的。

关闭 3-gram 后正确率有显著下降，提高 2-gram 权重也会有一定下降，可见 3-gram 还是一个比较科学的改进。

### 效率分析
考虑到算法的实现方式，从拼音到词的转换是通过一个 Trie 实现的，符合读音的算法认识的词不会太多，因此每一步枚举的词的长度并不会对算法性能有太大的影响。
而 词频、2-gram 和 3-gram 都是直接在 HashMap 上查找的，因此开关 2-gram 以及改变转移表大小也不会对性能有特别大的影响。

因此，无论如何调整参数，算法消耗的时间不会有很大变化。

对于空间而言，显然训练语料越多，算法可能接触到的词汇组合越多，得到的转移表文件越大。这个主要影响内存占用，以及训练、加载的时间。在现在的例子中，转移表存储文件 317M，反序列化后占用内存 2G，虽然里面大多数都是不怎么重要的内容（非常罕见的组合），但是因为没有做 Trimming 的动力，所以就留下来了..

同样地，在分词的时候如果使用了 Jieba 分词的 HMM 算法，也会让词库大大增加。考虑到我比较贫穷，SSD 买不起，而且家用计算机的内存限制，基本上这就是极限了。

## 总结
终于知道 HMM 是个什么东西了...

算法可改进的地方还有很多，上文提及的比较显然的：
- 删除掉转移表里比较小的项，可以显著降低模型的大小，加快启动速度，降低内存占用
- 加入对拼音本身频率的处理。有些字的读音基本上只在名字中出现，或者很少出现，所以权值还应该乘上这个字读这个音的概率。但是我没有找到这样的数据，所以这一次没有添加。
- 词性推断，保证句子结构完整

其中删除较小项比较好做，剩下两个由于数据缺失，或者比较复杂，都是比较难做的事情。但是如果完成了，就可以进一步降低转移表的大小。现在市面上实用的输入法引擎肯定都没有300M这么大，所以现在我的实现还是非常 Naive 的
